{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Bastian Brinksma\n",
    "\n",
    "Student ID: T14902110\n",
    "\n",
    "GitHub ID: BastianBrinksma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Phase Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) that considered as **phase 1 (from exercise 1 to exercise 15)**. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** up **until phase 1**. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    -  Use [the new dataset](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/newdataset/Reddit-stock-sentiment.csv). The dataset contains a 16 columns including 'text' and 'label', with the sentiment labels being: 1.0 is positive, 0.0 is neutral and -1.0 is negative. You can simplify the dataset and use only the columns that you think are necessary. \n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "    - Use this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 10% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    \n",
    "\n",
    "\n",
    "4. Fourth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (September 28th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Phase Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can keep the answer for phase 1 for easier running and update the phase 2 on the same page.**\n",
    "\n",
    "1. First: Continue doing the **take home** exercises in the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) for **phase 2, starting from Finding frequent patterns**. Use the same master(.ipynb) file. Answer from phase 1 will not be considered at this stage. You can answer in the master file. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: Continue from first phase and do the same process from the [DM2025-Lab1-Master](https://github.com/leoson-wu/DM2025-Lab1-Exercise/blob/main/DM2025-Lab1-Master.ipynb) on **the new dataset** for phase 2, starting from Finding frequent pattern. You can skip some exercises if you think some steps are not necessary. However main exercises should be completed. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 15% of your grade.__\n",
    "    - Continue using this file to complete the homework from the second part. Make sure the code can be run from the beginning till the end and has all the needed output. Use the same new dataset as in phase 1.\n",
    "    \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 20% of your grade.__\n",
    "    - Use this file to answer.\n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency).  Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences and when using augmentation with feature pattern.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 5% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [DM2025-Lab1-announcement](https://github.com/leoson-wu/DM2025-Lab1-Announcement/blob/main/README.md). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 19th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\basti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# test code for environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt') # download the NLTK datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly as py\n",
    "import math\n",
    "# If you get \"ModuleNotFoundError: No module named 'PAMI'\"\n",
    "# run the following in a new Jupyter cell:\n",
    "# !pip3 install PAMI\n",
    "import PAMI\n",
    "import umap\n",
    "\n",
    "import helpers.data_mining_helpers as dmh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\basti\\OneDrive\\AI Year 3 Taipei\\Data Mining\\DM2025Labs\\DM2025-Lab1-Exercise\\.venv\\Scripts\\python.exe\n",
      "3.13.6 (tags/v3.13.6:4e66535, Aug  6 2025, 14:36:00) [MSC v.1944 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable) # c:\\<your path to the project directory>\\.venv\\Scripts\\python.exe\n",
    "print(sys.version) #3.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Exercises 1-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did all the exercises in the Master.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 New Dataset Up Until Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type             datetime  post_id       subreddit  \\\n",
      "0  comment  2025-04-11 17:29:56  mmli62w  wallstreetbets   \n",
      "\n",
      "                                             title        author  \\\n",
      "0  Retardation is on the menu boys! WSB is so back  StickyTip420   \n",
      "\n",
      "                                    url  upvotes  downvotes  upvote_ratio  \\\n",
      "0  https://i.redd.it/0yq2ftren8ue1.jpeg        0        NaN           NaN   \n",
      "\n",
      "               text  subjectivity  polarity  sentiment entities  label  \n",
      "0  Calls on retards           1.0      -0.9       -1.0       []   -1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_11436\\3246695200.py:1: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  newdataset = pd.read_csv('newdataset\\Reddit-stock-sentiment.csv')\n"
     ]
    }
   ],
   "source": [
    "newdataset = pd.read_csv('newdataset\\Reddit-stock-sentiment.csv')\n",
    "print(newdataset.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                     Calls on retards\n",
      "1    Stunt as in like why did they even make a big ...\n",
      "2                    Seeing lots of red in the ticker.\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "print(newdataset['text'].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type         subreddit  \\\n",
      "0  comment    wallstreetbets   \n",
      "1  comment    wallstreetbets   \n",
      "2  comment       StockMarket   \n",
      "3     post  stockstobuytoday   \n",
      "4  comment       StockMarket   \n",
      "\n",
      "                                               title                author  \n",
      "0    Retardation is on the menu boys! WSB is so back          StickyTip420  \n",
      "1  Retail giant TARGET has now declined for 10 co...  Comfortable-Dog-8437  \n",
      "2  How do you feel about a sitting president maki...          Btankersly66  \n",
      "3                              Who knows more? $VMAR             emiljenfn  \n",
      "4  The Trump administration is begging Xi Jinping...          Just-Big6411  \n",
      "       type             datetime  post_id       subreddit  \\\n",
      "0   comment  2025-04-11 17:29:56  mmli62w  wallstreetbets   \n",
      "1   comment   2025-04-12 1:12:19  mmnu7v9  wallstreetbets   \n",
      "5   comment   2025-04-12 2:19:03  mmo4k9g  wallstreetbets   \n",
      "7   comment  2025-04-11 17:28:50  mmlhxwc  wallstreetbets   \n",
      "11  comment   2025-04-12 2:51:20  mmo9e6r  wallstreetbets   \n",
      "\n",
      "                                                title                author  \\\n",
      "0     Retardation is on the menu boys! WSB is so back          StickyTip420   \n",
      "1   Retail giant TARGET has now declined for 10 co...  Comfortable-Dog-8437   \n",
      "5   Weekend Discussion Thread for the Weekend of A...           PolarNimbus   \n",
      "7     Retardation is on the menu boys! WSB is so back             OSRSkarma   \n",
      "11  Weekend Discussion Thread for the Weekend of A...      RememberTooSmile   \n",
      "\n",
      "                                                  url  upvotes  downvotes  \\\n",
      "0                https://i.redd.it/0yq2ftren8ue1.jpeg        0        NaN   \n",
      "1                https://i.redd.it/7tl6puv9waue1.jpeg      -15        NaN   \n",
      "5   https://www.reddit.com/r/wallstreetbets/commen...       16        NaN   \n",
      "7                https://i.redd.it/0yq2ftren8ue1.jpeg       -3        NaN   \n",
      "11  https://www.reddit.com/r/wallstreetbets/commen...        9        NaN   \n",
      "\n",
      "    upvote_ratio                                               text  \\\n",
      "0            NaN                                   Calls on retards   \n",
      "1            NaN  Stunt as in like why did they even make a big ...   \n",
      "5            NaN  Hoping to ejaculate in wet warm puss tonight, ...   \n",
      "7            NaN  Confirmed not a trap. Its been like this for p...   \n",
      "11           NaN  this sub has ruined emails for me, whenever I ...   \n",
      "\n",
      "    subjectivity  polarity  sentiment                                entities  \\\n",
      "0       1.000000 -0.900000       -1.0                                      []   \n",
      "1       0.177778  0.083333        1.0   ['Stunt', 'company', 'deal', 'place']   \n",
      "5       0.500000  0.250000        1.0                     ['tonight', 'puss']   \n",
      "7       0.700000  0.050000        1.0  ['Confirmed', 'mouth', 'word', 'trap']   \n",
      "11      0.300000  1.000000        1.0                  ['sub', 'WSB', 'Best']   \n",
      "\n",
      "    label  \n",
      "0    -1.0  \n",
      "1     0.0  \n",
      "5     0.0  \n",
      "7     0.0  \n",
      "11    0.0  \n",
      "        type             datetime  post_id         subreddit  \\\n",
      "3       post  2023-08-30 17:12:55  165kllm  stockstobuytoday   \n",
      "5    comment   2025-04-12 2:19:03  mmo4k9g    wallstreetbets   \n",
      "19   comment   2025-04-11 4:32:16  mmig2h2            stocks   \n",
      "28   comment  2025-04-11 17:21:44  mmlgh35    wallstreetbets   \n",
      "42   comment  2021-02-01 13:45:17  gllicpw            stocks   \n",
      "..       ...                  ...      ...               ...   \n",
      "787  comment   2025-04-11 4:28:30  mmiflsp       StockMarket   \n",
      "793  comment   2025-04-11 3:44:29  mmi9ydd       StockMarket   \n",
      "820  comment  2025-04-11 15:19:11  mmkrha9            stocks   \n",
      "821  comment  2025-04-11 21:18:20  mmmr3ds    wallstreetbets   \n",
      "831  comment  2025-04-11 15:00:14  mmknn5s            stocks   \n",
      "\n",
      "                                                 title                author  \\\n",
      "3                                Who knows more? $VMAR             emiljenfn   \n",
      "5    Weekend Discussion Thread for the Weekend of A...           PolarNimbus   \n",
      "19   U.S. 10-year Treasury yields rise as Trump tar...              Vanman04   \n",
      "28     Retardation is on the menu boys! WSB is so back  PlannedObsolescence-   \n",
      "42   It's fucking awful seeing the \"Silver\" misinfo...          SmithRune735   \n",
      "..                                                 ...                   ...   \n",
      "787  Data Shows US Allies—Not China—Dumping Treasuries          whatproblems   \n",
      "793                     $ U.S. dollar value (crashing)              cdmpants   \n",
      "820  BlackRock’s Larry Fink says U.S. is very close...           hackslash74   \n",
      "821  Weekend Discussion Thread for the Weekend of A...       strangehitman22   \n",
      "831  US consumer sentiment plummets to second-lowes...  CulturalAtmosphere85   \n",
      "\n",
      "                                                   url  upvotes  downvotes  \\\n",
      "3    https://www.reddit.com/r/stockstobuytoday/comm...       30        0.0   \n",
      "5    https://www.reddit.com/r/wallstreetbets/commen...       16        NaN   \n",
      "19   https://www.reddit.com/r/stocks/comments/1jwfy...       12        NaN   \n",
      "28                https://i.redd.it/0yq2ftren8ue1.jpeg       53        NaN   \n",
      "42   https://www.reddit.com/r/stocks/comments/la34b...     3943        NaN   \n",
      "..                                                 ...      ...        ...   \n",
      "787               https://i.redd.it/31r650bss4ue1.jpeg       55        NaN   \n",
      "793               https://i.redd.it/atvlo83gk4ue1.jpeg       13        NaN   \n",
      "820  https://www.reddit.com/r/stocks/comments/1jwr1...       16        NaN   \n",
      "821  https://www.reddit.com/r/wallstreetbets/commen...       18        NaN   \n",
      "831  https://www.reddit.com/r/stocks/comments/1jwr3...       12        NaN   \n",
      "\n",
      "     upvote_ratio                                               text  \\\n",
      "3            0.98  Vision Marine Technologies Inc. is rewriting t...   \n",
      "5             NaN  Hoping to ejaculate in wet warm puss tonight, ...   \n",
      "19            NaN  I think we are past the numbers. Just assume f...   \n",
      "28            NaN       My first wife was tarded, she's a pilot now.   \n",
      "42            NaN  So they went from saying redditors are bad for...   \n",
      "..            ...                                                ...   \n",
      "787           NaN  good reminder there’s a bigger game than just ...   \n",
      "793           NaN                 How does this drop interest rates?   \n",
      "820           NaN         Moving away from recession into depression   \n",
      "821           NaN  Lost 2k first week \"investing\" in options, dis...   \n",
      "831           NaN  Things will get better soon. Once those 9 tran...   \n",
      "\n",
      "     subjectivity  polarity  sentiment  \\\n",
      "3        0.646970  0.216383        1.0   \n",
      "5        0.500000  0.250000        1.0   \n",
      "19       0.451956  0.029337        1.0   \n",
      "28       0.333333  0.250000        1.0   \n",
      "42       0.666667 -0.700000       -1.0   \n",
      "..            ...       ...        ...   \n",
      "787      0.480000 -0.100000       -1.0   \n",
      "793      0.000000  0.000000        0.0   \n",
      "820      0.000000  0.000000        0.0   \n",
      "821      0.316667  0.025000        1.0   \n",
      "831      0.260000  0.165000        1.0   \n",
      "\n",
      "                                              entities  label  \n",
      "3    ['watercraft', 'skill', 'power', ']', 'feat', ...    1.0  \n",
      "5                                  ['tonight', 'puss']    0.0  \n",
      "19   ['Hell', 'Far', 'stage', 'US', 'economy', 'par...   -1.0  \n",
      "28                                   ['pilot', 'wife']    0.0  \n",
      "42                                ['market', 'silver']    1.0  \n",
      "..                                                 ...    ...  \n",
      "787  ['idiot', 'picture', 't', 'deficits…', '’', 'r...    1.0  \n",
      "793                               ['interest', 'drop']    0.0  \n",
      "820                        ['recession', 'depression']   -1.0  \n",
      "821                     ['investing', 'chill', 'week']   -1.0  \n",
      "831  ['/s', 'economy', 'nothing', 'roaring', 'school']    1.0  \n",
      "\n",
      "[113 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "# Only show type, subreddit, title, author\n",
    "get_columns = newdataset[['type', 'subreddit', 'title', 'author']].head(5)\n",
    "print(get_columns)\n",
    "\n",
    "# Only comments from \"wallstreetbets\"\n",
    "wsb_comments = newdataset[newdataset['subreddit'] == 'wallstreetbets']\n",
    "print(wsb_comments.head(5))\n",
    "\n",
    "# Posts with more than 10 upvotes\n",
    "popular_posts = newdataset[newdataset['upvotes'] > 10]\n",
    "print(popular_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        type             datetime  post_id       subreddit  \\\n",
      "0    comment  2025-04-11 17:29:56  mmli62w  wallstreetbets   \n",
      "40   comment  2025-04-11 22:00:36  mmmyq7m  wallstreetbets   \n",
      "96   comment   2025-04-12 2:05:07  mmo2f48  wallstreetbets   \n",
      "183  comment  2025-04-11 22:45:45  mmn6eil  wallstreetbets   \n",
      "227  comment  2025-04-11 20:25:56  mmmh2t3  wallstreetbets   \n",
      "\n",
      "                                                 title              author  \\\n",
      "0      Retardation is on the menu boys! WSB is so back        StickyTip420   \n",
      "40   Weekend Discussion Thread for the Weekend of A...   InstructionNo4546   \n",
      "96                    Someone post the hotline please.           Rosie3435   \n",
      "183  Weekend Discussion Thread for the Weekend of A...  NonsensicalWorries   \n",
      "227  Weekend Discussion Thread for the Weekend of A...           VisualMod   \n",
      "\n",
      "                                                   url  upvotes  downvotes  \\\n",
      "0                 https://i.redd.it/0yq2ftren8ue1.jpeg        0        NaN   \n",
      "40   https://www.reddit.com/r/wallstreetbets/commen...       10        NaN   \n",
      "96                https://i.redd.it/tcfuu97p7bue1.jpeg        1        NaN   \n",
      "183  https://www.reddit.com/r/wallstreetbets/commen...       13        NaN   \n",
      "227  https://www.reddit.com/r/wallstreetbets/commen...        9        NaN   \n",
      "\n",
      "     upvote_ratio                                               text  \\\n",
      "0             NaN                                   Calls on retards   \n",
      "40            NaN  These China AI factory memes are pretty good. ...   \n",
      "96            NaN                        200k loss in a month.. haha   \n",
      "183           NaN  My resume isn't even read by companies anymore...   \n",
      "227           NaN  #Ban Bet Lost\\r\\n\\r\\n/u/Local-Wall-4359 made a...   \n",
      "\n",
      "     subjectivity  polarity  sentiment  \\\n",
      "0             1.0    -0.900       -1.0   \n",
      "40            0.8     0.475        1.0   \n",
      "96            0.3     0.200        1.0   \n",
      "183           0.0     0.000        0.0   \n",
      "227           0.4     0.800        1.0   \n",
      "\n",
      "                                              entities  label  \n",
      "0                                                   []   -1.0  \n",
      "40   ['factory', 'time', 'AI', 'China AI', 'China',...    0.0  \n",
      "96                     ['..', 'month', 'loss', 'haha']   -1.0  \n",
      "183                                         ['resume']   -1.0  \n",
      "227  ['Ban', 'Lost', '/u/Local-Wall-4359', 'WSB', '...    0.0  \n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "\n",
    "# Filter for a specific category \"wallstreetbets\"\n",
    "category_df = newdataset[newdataset['subreddit'] == 'wallstreetbets']\n",
    "\n",
    "# Select every 10th record\n",
    "every_10th = category_df.iloc[::10]\n",
    "\n",
    "# Show first 5 rows\n",
    "print(every_10th.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      (The amoung of missing records is: , 2)\n",
      "1      (The amoung of missing records is: , 2)\n",
      "2      (The amoung of missing records is: , 2)\n",
      "3      (The amoung of missing records is: , 0)\n",
      "4      (The amoung of missing records is: , 2)\n",
      "                        ...                   \n",
      "842    (The amoung of missing records is: , 2)\n",
      "843    (The amoung of missing records is: , 2)\n",
      "844    (The amoung of missing records is: , 0)\n",
      "845    (The amoung of missing records is: , 2)\n",
      "846    (The amoung of missing records is: , 2)\n",
      "Length: 847, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4\n",
    "# Check missing values row by row using your helper function\n",
    "missing_info = newdataset.isnull().apply(lambda row: dmh.check_missing_values(row), axis=1)\n",
    "\n",
    "# Show the first few results\n",
    "print(missing_info)\n",
    "\n",
    "len(newdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5\n",
    "# Not necessary, explained already in the Master.ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Data Visualizations for the new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part will be about the visualizations for the new dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
